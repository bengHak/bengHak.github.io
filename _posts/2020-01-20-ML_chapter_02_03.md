---
layout: post
title: 적응형 선형 뉴런(ADAptive LInear NEuron, ADALINE)과 학습의 수렴
subtitle: 머신러닝 교과서 with 파이썬,사이킷런,텐서플로 세바스찬 라시카, 바히드 미자리리 지음, 박해선 옮김
tags: [study, machinelearning, deeplearning, ML, review, anaconda, DSC, 머신러닝교과서2장]
comments: true
use_math: true
---
<img src="../img/ml_review/ml_book.jpg" width="40%" alt="머신러닝 교과서"/>

**한국외대 DSC(Developer Student Club)** 멤버들과 함께 머신러닝 스터디를 진행하면서 "**머신 러닝 교과서**"를 블로그에 정리한 내용이다.

---

2장에서는 분류를 위한 초창기 머신 러닝 알고리즘인 **퍼셉트론**과 **적응형 선형 뉴런** 두개를 사용한다. 이 두 알고리즘에 대해 이해하고, 파이썬을 사용한 효율적인 구현 방법을 익히는데 도움이 될 것이라고 한다. 아래 3가지를 2장에서 주로 다룬다고 한다.

- 머신 러닝 알고리즘을 직관적으로 이해하기
- Pandas, Numpy, Matplotlib으로 데이터를 읽고 처리하고 시각화하기
- 파이썬으로 선형 분류 알고리즘 구현하기

---

# 적응형 선형 뉴런(ADAptive LInear NEuron, ADALINE)

퍼셉트론 알고리즘이 등장한 지 몇 년이 되지 지나지 않아 버나드 위드로우(Bernard Widrow)와 그의 박사 과정 학생 테드 호프(Tedd Hoff)가 **아달린**(Adaline)을 발표했다. 아달린은 퍼셉트론의 향상된 버전으로 볼 수 있다.

**아달린**은 연속 함수(continuous function)로 비용 함수를 정의하고 최소화 한다.

**아달린** 규칙(위드로우-호프 규칙이라고도 함)과 로젠블라트 퍼셉트론의 가장 큰 차이점은 가중치를 업데이트하는 데 퍼셉트론처럼 단위 계단 함수 대신 **선형 활성화 함수**를 사용한다는 것이다. 선형 활성화 함수 $\phi(z)$ 는 최종 입력과 동일한 함수이다.  

$\phi(w^Tx)=w^Tx$

하지만 최종 예측은 임계 함수를 사용한다.

> 퍼셉트론과 아달린 알고리즘의 비교

<img src="https://thebook.io/img/007022/p065.jpg" alt="difference-between-adaline-perceptron">

> 출처: https://thebook.io/007022/ch02/03-01/

  

**오차 계산과 가중치 업데이트하는 방법**

- **아달린** 알고리즘: 진짜 클래스 레이블과 선형 활성화 함수의 실수 출력 값을 비교
- **퍼셉트론** 알고리즘: 진짜 클래스 레이블과 예측 클래스 레이블과 비교

  

---

# 경사 하강법으로 비용 함수 최소화

**목적 함수**는 지도 학습 알고리즘의 핵심 구성 요소로 학습 과정 동안 최적화하기 위해 정의된다. 종종 최소화하려는 비용 함수가 목적 함수가 된다. 아달린은 **제곱 오차함**(Sum of Squared Errors, SSE)로 가중치를 학습할 비용 함수 $J$르를 정의한다.  
$$
J(w) = {\frac{1}{2}}{\Sigma}_i(y^{(i)}-\phi(z^{i}))^2
$$


$\frac{1}{2}$ 항은 미분식을 간소하게 만들려고 편의상 추가한 것이다. 단위 계단 함수와 다르게 연속 함수는 미분 가능하다는 특징을 갖고 있다. 아래의 그림은 경사 하강법의 핵심 아이디어를 묘사한다.

> 경사 하강법 알고리즘

<img src="https://thebook.io/img/007022/p066.jpg" alt="gradient-descent-algorithm"/>

> 출처: https://thebook.io/007022/ch02/03/01/

수학적 **그래디언트**와 미분값은 구분된다. 수학적 표현으로 기울기를  $\nabla$ (gradient)라고 표현한다. 여기서 gradient는 스칼라를 벡터로 미분한 것이다.

경사 하강법은 아래와 같이 n번의 스텝을 통해 가중치를 업데이트 한다. $\alpha$ 는 학습률을 의미한다.

**1-D의 경우**  
$$
x_n = x_{n-1} - \alpha\frac{df(x_{n-1})}{dx}
$$
  

**N-D의 경우**  
$$
x_n = x_{n-1}-\alpha\nabla f(x_{n-1})
$$
  

아달린 학습 규칙이 퍼셉트론 규칙과 동일하게 보이지만 $z^{(i)}=w^Tx^{(i)}$인 $\phi(z^{(i)})$ 는 정수 클래스 레이블이 아니고 실수이다. 또 훈련 세트에 있는 모든 샘플을 기반으로 가중치 업데이트를 계산한다.(각 샘플마다 가중치를 업데이트 하지 않는다.) 이 방식을 **배치 경사 하강법**(batch gradient descent) 이라고 한다.

  

# 파이썬으로 아달린 구현





